<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
    <title>GenAI with Langchain4j and Ollama</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/theme/sky.min.css">
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css">
    <style>
        /* Keep existing heading text-transform override */
        h1, h2, h3, h4, h5, h6 {
            text-transform: none !important; /* Remove uppercase styling */
        }

        /* Remove extra lines from <pre> and <code> */
        pre, code {
            margin: 0 !important;
            padding: 0 !important;
            font-size: 0.8em !important; /* Adjust font size as needed */
            line-height: 1; /* Improve readability */
        }

        /* Enable auto-resize for sections */
        .reveal section {
            overflow: auto !important; /* Enable scrolling for overflowing content */
            padding: 0; /* Remove extra padding */
        }

        /* Prevent unnecessary space in syntax highlighting */
        code[class*="language-"] {
            white-space: pre !important; /* Maintain preformatted text */
        }

        ul ul {
            font-size: 80%;
        }

    </style>

</head>
<body>
<div class="reveal">
    <div class="slides">
        <section>
            <h3>GenAI with Langchain4j and Ollama</h3>
            <p>Helsinki Java meetup</p>
            <p>19 Feb 2025</p>
            <p>Nikolay Kuznetsov</p>
            <p><a href="https://github.com/nikolayk812">@nikolayk812</a></p>
        </section>


        <section>
            <h2>About me</h2>
            <p>Senior software engineer</p>
            <p>Pre-owned project at <span style="color: orange;">Zalando</span> Helsinki</p>
            <p>C → Java → Kotlin → Go</p>
            <p>Author of <i>pgx-outbox</i> library </p>
        </section>

        <section>
            <h2>Previous experience with AI</h2>
            <p>ChatGPT, GitHub Copilot, Claude</p>
            <p>
                Pet project with OpenAI API calls
            </p>
        </section>

        <section>
            <h2>Ollama</h2>
            <p>Like Docker but for LLMs</p>
            <p>Local inference, minimal setup</p>
            <p>Written in Go, uses <i>llama.cpp</i></p>
            <p>126K stars at GitHub</p>

            <img src="images/ollama_logo.png" alt=""
                 style="position: absolute; top: 50%; left: 85%; transform: translate(-50%, -50%); width: 200px;">
        </section>

        <section>
            <h2>Stargazers</h2>
            <p>OpenJDK 20K</p>
            <p>Kotlin 50K</p>
            <p>Spring 57K</p>
            <p>Kubernetes 113K</p>
            <p>Golang 125.7K</p>

            <img src="images/stargazer.png" alt=""
                 style="position: absolute; top: 50%; left: 85%; transform: translate(-50%, -50%); width: 250px;">
        </section>

        <section>
            <h2>Ollama demo</h2>
            <pre><code class="language-bash">
        > ollama pull llama3.2

        > ollama run llama3.2

        > ollama ps

        >>> Please, briefly compare Java and Go

        >>> /bye
            </code></pre>
        </section>

        <section>
            <h2>Supported models</h2>
            <p>Llama by Meta</p>
            <p>Phi by Microsoft</p>
            <p>Mistral</p>
            <p>Gemma by Google</p>
            <p>Qwen by Alibaba</p>
            <p>DeepSeek</p>
        </section>

        <section>
            <h2>Ollama APIs</h2>
            <p>Ollama-style: <i>/api/generate</i></p>

            <pre><code class="language-bash">
        curl -X POST http://localhost:11434/api/generate \
             -H "Content-Type: application/json" \
             -d '{
               "model": "llama3.2",
               "prompt": "Please, briefly compare Java and Go",
               "stream": false
             }'
            </code></pre>

            <p>OpenAI-compatible: <i>/v1/chat/completions</i></p>
        </section>

        <section>
            <h2>Talking to LLMs</h2>
            <p><b>Langchain4j</b></p>
            <p>Spring AI</p>
            <p>Official SDKs: OpenAI, AWS, etc</p>

        </section>

        <section>
            <h2>Langchain4j + Ollama </h2>
            <pre><code class="language-java">
        import dev.langchain4j.model.chat.ChatLanguageModel;
        import dev.langchain4j.model.ollama.OllamaChatModel;

        ChatLanguageModel model = OllamaChatModel.builder()
                .baseUrl("http://localhost:11434")
                .modelName("llama3.2:1b")
                .build();

        var answer = model.chat("Why Java is still awesome in 2025?");

        log.info("Response from LLM -> {}", answer);
            </code></pre>
        </section>

        <section>
            <h2>Chat Language Model</h2>
            <pre><code class="language-java">
        package dev.langchain4j.model.chat;


        public interface ChatLanguageModel {

            // main API to interact with a chat model
            default ChatResponse chat(ChatRequest chatRequest)

            default String chat(String userMessage)
            </code></pre>
        </section>

        <section>
            <h2>Langchain4j + OpenAI </h2>
            <pre><code class="language-java">
        import dev.langchain4j.model.openai.OpenAiChatModel;

        var model = OpenAiChatModel.builder()
                .apiKey(System.getenv("OPENAI_API_KEY"))
                .modelName("gpt-4o")
                .build();

        var answer = model.chat("Why Java is still awesome in 2025?");
            </code></pre>
        </section>

        <section>
            <h2>Langchain4j demo</h2>
            <img src="images/langchain4j_demo.png" alt="" width="700">
        </section>

        <section>
            <h2>Streaming model</h2>
            <pre><code class="language-java">
    var model = OllamaStreamingChatModel.builder().baseUrl("http://localhost:11434").modelName("llama3.2").build();

    model.chat("Provide a long explanation why Java is awesome",
            new StreamingChatResponseHandler() {
                public void onPartialResponse(String token) {
                    out.print(token);
                }

                public void onError(Throwable error) {
                    // handle error
                }

                public void onCompleteResponse(ChatResponse response) {
                    out.println();
                }
            });
            </code></pre>
        </section>

        <section>
            <h2>Streaming pitfalls</h2>
            <p>Langchain4j is still 1.0.0-beta1</p>
            <p><i>model.chat()</i> blocks for OpenAI</p>
            <p><i>model.chat()</i> is async call for Ollama</p>
        </section>

        <section>
            <h2>Streaming demo</h2>
            <p>Using <i>CompletableFuture</i></p>
            <p>and <i>Thread.startVirtualThread</i></p>
            <p>Server-Sent Events (SSE)</p>
        </section>

        <section>
            <h2>Chat</h2>
            <pre><code class="language-java">
    var scanner = new Scanner(System.in);
    var conversation = synchronizedList(new ArrayList&lt;ChatMessage&gt;());

    while (true) {
        var userInput = scanner.nextLine();
        conversation.add(UserMessage.from(userInput));

        model.generate(conversation,
            new StreamingResponseHandler<>() {
                public void onComplete(Response&lt;AiMessage&gt; response) {
                    conversation.add(response.content());
                    out.println("You: ");
                }
        });
    }
            </code></pre>
        </section>

        <section>
            <h2>Chat demo</h2>
            <p>Conversation history in each request</p>
            <p>Watch out for <i>context length / window</i></p>
            <p>(128K tokens for Llama 3.2)</p>
        </section>

        <section>
            <h2>Image recognition</h2>
            <pre><code class="language-java">
        import dev.langchain4j.data.message.*;


        var userMessage = UserMessage.from(
            TextContent.from("What do you see?"),
            ImageContent.from(
                readImageInBase64("/computer.jpeg"),
                "image/jpeg"
            )
        );

        var response = model.chat(userMessage);
            </code></pre>
            <p>moondream is the tiniest model</p>
        </section>

        <section>
            <h2>Image recognition demo</h2>
            <img src="images/computer.jpeg" alt="">
            <pre><code class="language-text">
        The image features a desktop computer setup with a monitor,
        keyboard, and mouse. The monitor is placed on the right side
        of the desk, while the keyboard is situated in front of it.
            </code></pre>
        </section>

        <section>
            <h3><i>Augmented Generation (AG)</i></h3>
            <p>"prompt engineering" at server side</p>
        </section>

        <section>
            <h3><i>Embeddings</i></h3>
            <p>numerical vectors that represent</p>
            <p>the semantic meaning of data</p>
            <p>in a high-dimensional space</p>
        </section>

        <section>
            <h2>Embeddings</h2>
            <pre><code class="language-java">
    import dev.langchain4j.data.embedding.Embedding;
    import dev.langchain4j.store.embedding.CosineSimilarity;

    // model is nomic-embed-text
    Embedding cat = model.embed("Cat is domesticated animal").content();
    Embedding ollama = model.embed("Ollama runs LLMs locally").content();

    // float[768] vector; normalized hence: [-1, 1]

    double similarity = CosineSimilarity.between(cat, ollama);
    log.info("Cosine similarity between embeddings is: {}", similarity);
            </code></pre>
        </section>

        <section>
            <img src="images/embedding.png" alt="" width="700">
        </section>

        <section>
            <h3><i>Retrieval Augmented Generation (RAG)</i></h3>
            <p>"prompt engineering" at server side</p>
            <p>using data retrieved from a (vector) database</p>
        </section>

        <section>
            <h2>Vector database</h2>
            <p>specialized storage for embeddings</p>
            <p>and efficient similarity-searching</p>
        </section>

        <section>
            <h2>Vector databases</h2>
            <p>prototyping: FAISS and ChromaDB</p>
            <p>managed: Pinecone and Weaviate</p>
            <p>large-scale: Milvus and Qdrant</p>
        </section>

        <section>
            <h2>Credits</h2>
            <p><b>Ignacio López Luna</b>:</p>
            <p><a href="https://github.com/ilopezluna/generative-ai-with-testcontainers">ilopezluna/generative-ai-with-testcontainers</a>
            </p>

            <p><b>Manuel de la Peña</b>:</p>
            <p><a href="https://github.com/mdelapenya/generative-ai-with-testcontainers">mdelapenya/generative-ai-with-testcontainers</a>
            </p>
        </section>

        <section>
            <h2>Testcontainers</h2>
            <p>Favorite: Postgres, Localstack</p>
            <p>Used: ToxiProxy, Redis, Elasticsearch</p>
            <p>Ollama can be run in Testcontainers!</p>

            <img src="images/testcontainers.png" alt=""
                 style="position: absolute; top: 50%; left: 88%; transform: translate(-50%, -50%); width: 200px;">
        </section>

        <section>
            <h2>Thank you!</h2>
            <p><a href="https://github.com/nikolayk812/genai-java">github.com/nikolayk812/genai-java</a></p>
        </section>

    </div>
</div>

<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
<script>
    Reveal.initialize({
        width: "89%", // Use a percentage to make slides responsive
        height: "89%",
        controls: true,
        progress: true,
        slideNumber: true,
        history: true,
        center: true,
        transition: 'slide', // none/fade/slide/convex/concave/zoom
    });

    hljs.highlightAll(); // Initialize code highlighting
</script>
</body>
</html>
